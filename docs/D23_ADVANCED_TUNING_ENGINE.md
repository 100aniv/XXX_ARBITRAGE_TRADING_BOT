
# D23 Advanced Tuning Engine Guide

**Document Version:** 1.0  
**Date:** 2025-11-16  
**Status:** âœ… Complete  

---

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [ì•„í‚¤í…ì²˜](#ì•„í‚¤í…ì²˜)
3. [Optimizer ì¶”ìƒí™”](#optimizer-ì¶”ìƒí™”)
4. [êµ¬í˜„ëœ ìµœì í™” ì•Œê³ ë¦¬ì¦˜](#êµ¬í˜„ëœ-ìµœì í™”-ì•Œê³ ë¦¬ì¦˜)
5. [D22 Tuning Harness í†µí•©](#d22-tuning-harness-í†µí•©)
6. [ì„¤ì • ë° ì‹¤í–‰](#ì„¤ì •-ë°-ì‹¤í–‰)
7. [Observability ì •ì±…](#observability-ì •ì±…)
8. [í–¥í›„ ê³„íš](#í–¥í›„-ê³„íš)

---

## ê°œìš”

D23ì€ **Advanced Tuning Engine**ì„ êµ¬í˜„í•©ë‹ˆë‹¤. íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì„ ì§€ì›í•˜ëŠ” í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### í•µì‹¬ íŠ¹ì§•

- âœ… **Optimizer ì¶”ìƒí™”**: BaseOptimizer ì¸í„°í˜ì´ìŠ¤
- âœ… **Grid Search**: ê²°ì •ë¡ ì  ê·¸ë¦¬ë“œ íƒìƒ‰
- âœ… **Random Search**: ê· ì¼ ë¶„í¬ ìƒ˜í”Œë§
- âœ… **Bayesian Optimization**: êµ¬ì¡° ê¸°ë°˜ ì„¤ê³„ (í–¥í›„ í™•ì¥)
- âœ… **D22 Tuning Harness í†µí•©**: ê¸°ì¡´ íŠœë‹ ì‹œìŠ¤í…œê³¼ í˜¸í™˜
- âœ… **StateManager ì—°ë™**: Redis ê¸°ë°˜ ê²°ê³¼ ì €ì¥
- âœ… **Observability ì •ì±… ì¤€ìˆ˜**: ê°€ì§œ ë©”íŠ¸ë¦­ ì—†ìŒ

### ì„¤ê³„ ì›ì¹™

1. **ì‹¤ì œ ìµœì í™” ì—†ìŒ**: êµ¬ì¡°ì™€ ì¸í„°í˜ì´ìŠ¤ë§Œ êµ¬í˜„
2. **í”ŒëŸ¬ê·¸ì¸ ê°€ëŠ¥**: í–¥í›„ scikit-opt, hyperopt, optuna ë“± ì—°ë™ ê°€ëŠ¥
3. **í…ŒìŠ¤íŠ¸ ê°€ëŠ¥**: ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ Mock ê°€ëŠ¥
4. **ì¸í”„ë¼ ì•ˆì „**: StateManagerë¥¼ í†µí•œ Redis ì ‘ê·¼ë§Œ ì‚¬ìš©

---

## ì•„í‚¤í…ì²˜

### ê³„ì¸µ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      TuningHarness (D22)                â”‚
â”‚   - ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œ                        â”‚
â”‚   - ëª©ì  í•¨ìˆ˜ ì‹¤í–‰                       â”‚
â”‚   - ê²°ê³¼ ì €ì¥                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Optimizer (D23)                    â”‚
â”‚   - ask(): íŒŒë¼ë¯¸í„° ì œì•ˆ                 â”‚
â”‚   - tell(): ê²°ê³¼ ê¸°ë¡                    â”‚
â”‚   - get_history(): íˆìŠ¤í† ë¦¬ ì¡°íšŒ         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”
        â–¼      â–¼      â–¼
    â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Gridâ”‚ â”‚Randâ”‚ â”‚Bayesianâ”‚
    â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì£¼ìš” í´ë˜ìŠ¤

#### 1. TuningMethod (Enum)

```python
class TuningMethod(Enum):
    GRID = "grid"
    RANDOM = "random"
    BAYESIAN = "bayesian"
```

#### 2. ParameterBound

íŒŒë¼ë¯¸í„° ë²”ìœ„ ì •ì˜ ë° ê²€ì¦:

```python
@dataclass
class ParameterBound:
    name: str              # íŒŒë¼ë¯¸í„° ì´ë¦„
    param_type: str        # "float" ë˜ëŠ” "int"
    bounds: Tuple[float, float]  # (min, max)
    
    def validate(self, value: Any) -> bool:
        """ê°’ì´ ë²”ìœ„ ë‚´ì¸ì§€ í™•ì¸"""
```

#### 3. BaseOptimizer (ì¶”ìƒ í´ë˜ìŠ¤)

ëª¨ë“  ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì˜ ê¸°ë³¸ ì¸í„°í˜ì´ìŠ¤:

```python
class BaseOptimizer(ABC):
    def __init__(self, search_space: List[ParameterBound]):
        ...
    
    @abstractmethod
    def ask(self) -> Dict[str, Any]:
        """ë‹¤ìŒ ì‹œë„í•  íŒŒë¼ë¯¸í„° ì œì•ˆ"""
        pass
    
    @abstractmethod
    def tell(self, params: Dict[str, Any], result_summary: Dict[str, Any]) -> None:
        """ê²°ê³¼ ê¸°ë¡ ë° ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        pass
    
    def get_history(self) -> List[OptimizationResult]:
        """ìµœì í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        pass
```

---

## Optimizer ì¶”ìƒí™”

### ask() / tell() íŒ¨í„´

ëª¨ë“  OptimizerëŠ” ë‹¤ìŒ íŒ¨í„´ì„ ë”°ë¦…ë‹ˆë‹¤:

```python
# ì´ˆê¸°í™”
optimizer = create_optimizer(TuningMethod.BAYESIAN, search_space)

# ë°˜ë³µ
for iteration in range(max_iterations):
    # 1. íŒŒë¼ë¯¸í„° ì œì•ˆ
    params = optimizer.ask()
    
    # 2. ëª©ì  í•¨ìˆ˜ ì‹¤í–‰ (ì™¸ë¶€)
    result = objective_function(params)
    
    # 3. ê²°ê³¼ ê¸°ë¡
    optimizer.tell(params, result)
```

### ê²°ê³¼ ìŠ¤í‚¤ë§ˆ

```python
@dataclass
class OptimizationResult:
    iteration: int                          # ë°˜ë³µ ë²ˆí˜¸
    params: Dict[str, Any]                  # ì‚¬ìš©í•œ íŒŒë¼ë¯¸í„°
    result_summary: Dict[str, Any]          # ê²°ê³¼ ìš”ì•½ (í‚¤ë§Œ)
    timestamp: str                          # ISO8601 íƒ€ì„ìŠ¤íƒ¬í”„
```

**result_summary ì˜ˆì‹œ (ìˆ«ì ì—†ìŒ):**

```python
{
    "status": "completed",
    "scenario": "basic_spread_win",
    "engine_mode": "paper"
}
```

---

## êµ¬í˜„ëœ ìµœì í™” ì•Œê³ ë¦¬ì¦˜

### 1. GridOptimizer

**íŠ¹ì§•:**
- ê²°ì •ë¡ ì  ê·¸ë¦¬ë“œ íƒìƒ‰
- ê° ì°¨ì›ë‹¹ `grid_points` ê°œì˜ í¬ì¸íŠ¸ ìƒì„±
- ëª¨ë“  ì¡°í•© íƒìƒ‰

**ì‚¬ìš© ì˜ˆ:**

```python
optimizer = GridOptimizer(
    search_space=[
        ParameterBound("min_spread_pct", "float", (0.05, 0.50)),
        ParameterBound("slippage_bps", "int", (1, 30))
    ],
    grid_points=3
)
```

**ë™ì‘:**
- ì²« ë²ˆì§¸ ask(): (0.05, 1)
- ë‘ ë²ˆì§¸ ask(): (0.05, 15)
- ì„¸ ë²ˆì§¸ ask(): (0.05, 30)
- ...

### 2. RandomOptimizer

**íŠ¹ì§•:**
- ê· ì¼ ë¶„í¬ ìƒ˜í”Œë§
- ì‹œë“œ ê¸°ë°˜ ì¬í˜„ì„±
- ë¹ ë¥¸ íƒìƒ‰

**ì‚¬ìš© ì˜ˆ:**

```python
optimizer = RandomOptimizer(
    search_space=[...],
    seed=42
)
```

**ë™ì‘:**
- ask(): ë²”ìœ„ ë‚´ ë¬´ì‘ìœ„ ê°’ ë°˜í™˜
- ê°™ì€ ì‹œë“œ â†’ ê°™ì€ ì‹œí€€ìŠ¤ (ì¬í˜„ì„±)

### 3. BayesianOptimizer

**í˜„ì¬ ìƒíƒœ:**
- êµ¬ì¡°ë§Œ êµ¬í˜„ (ì‹¤ì œ GP ëª¨ë¸ ì—†ìŒ)
- Random sampling ì‚¬ìš© (ask())
- íˆìŠ¤í† ë¦¬ ì €ì¥ (tell())

**í–¥í›„ í™•ì¥ (D24+):**
- Gaussian Process ëª¨ë¸
- Acquisition function (UCB, EI, POI)
- scikit-opt / hyperopt / optuna í†µí•©

**ì‚¬ìš© ì˜ˆ:**

```python
optimizer = BayesianOptimizer(
    search_space=[...],
    acquisition_fn="ucb",
    seed=42
)
```

---

## D22 Tuning Harness í†µí•©

### TuningHarness í´ë˜ìŠ¤

```python
class TuningHarness:
    def __init__(self, config: TuningConfig, state_manager: Optional[StateManager] = None):
        """
        Args:
            config: íŠœë‹ ì„¤ì •
            state_manager: StateManager (Redis ì—°ë™)
        """
        ...
    
    def run_iteration(self, iteration: int, objective_fn) -> Dict[str, Any]:
        """í•œ ë²ˆì˜ íŠœë‹ ë°˜ë³µ ì‹¤í–‰"""
        params = self.optimizer.ask()
        result = objective_fn(params)
        self.optimizer.tell(params, result)
        return result
```

### í†µí•© íë¦„

```python
# 1. ì„¤ì • ë¡œë“œ
config = load_tuning_config("configs/d23_tuning/advanced_baseline.yaml")

# 2. Harness ìƒì„±
harness = TuningHarness(config)

# 3. ëª©ì  í•¨ìˆ˜ ì •ì˜
def objective_function(params):
    # ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰
    # ê²°ê³¼ ê³„ì‚°
    return {"status": "completed"}

# 4. ë°˜ë³µ ì‹¤í–‰
for i in range(config.max_iterations):
    result = harness.run_iteration(i + 1, objective_function)

# 5. ê²°ê³¼ ì¡°íšŒ
results = harness.get_results()
history = harness.get_optimizer_history()
```

### StateManager ì—°ë™

ê²°ê³¼ëŠ” ìë™ìœ¼ë¡œ Redisì— ì €ì¥ë©ë‹ˆë‹¤:

```python
# Namespace: tuning:{env}:{mode}
# Key: tuning:{env}:{mode}:arbitrage:tuning_result:{iteration}

{
    "iteration": "1",
    "params": "{'min_spread_pct': 0.1, ...}",
    "timestamp": "2025-11-16T12:34:56.789123"
}
```

---

## ì„¤ì • ë° ì‹¤í–‰

### ì„¤ì • íŒŒì¼ êµ¬ì¡°

**File:** `configs/d23_tuning/advanced_baseline.yaml`

```yaml
tuning:
  # íŠœë‹ ë°©ë²•
  method: bayesian  # grid, random, bayesian
  
  # ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼
  scenarios:
    - configs/d17_scenarios/basic_spread_win.yaml
    - configs/d17_scenarios/choppy_market.yaml
  
  # íŒŒë¼ë¯¸í„° ë²”ìœ„
  search_space:
    min_spread_pct:
      type: float
      bounds: [0.05, 0.50]
    
    slippage_bps:
      type: int
      bounds: [1, 30]
    
    max_position_krw:
      type: int
      bounds: [300000, 2000000]
  
  # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
  max_iterations: 10
  
  # ë‚œìˆ˜ ì‹œë“œ
  seed: 42
  
  # Grid Search í¬ì¸íŠ¸ ìˆ˜
  grid_points: 3
  
  # Acquisition function
  acquisition_fn: ucb
```

### ì‹¤í–‰ ëª…ë ¹

```bash
# CLI ìŠ¤í¬ë¦½íŠ¸ (í–¥í›„ êµ¬í˜„)
python scripts/run_d23_tuning.py --config configs/d23_tuning/advanced_baseline.yaml

# ë˜ëŠ” Python ì½”ë“œ
python -c "
from arbitrage.tuning import load_tuning_config, TuningHarness

config = load_tuning_config('configs/d23_tuning/advanced_baseline.yaml')
harness = TuningHarness(config)

def objective(params):
    return {'status': 'completed'}

for i in range(3):
    harness.run_iteration(i + 1, objective)

print(f'Completed {len(harness.get_results())} iterations')
"
```

---

## Observability ì •ì±…

### ì •ì±… ëª…ì‹œ

**For all tuning / optimization scripts,
this project NEVER documents fake or "expected" outputs with concrete numbers.
Only real logs from actual executions may be shown in reports, otherwise we only describe the format and fields conceptually.**

### ì¤€ìˆ˜ ì‚¬í•­

1. âŒ "ì˜ˆìƒ ê²°ê³¼", "ìƒ˜í”Œ PnL", "ê¸°ëŒ€ ìˆ˜ìµë¥ " ê¸ˆì§€
2. âŒ êµ¬ì²´ì ì¸ ìˆ«ìê°€ í¬í•¨ëœ ì¶œë ¥ ì˜ˆì‹œ ê¸ˆì§€
3. âœ… ì•Œê³ ë¦¬ì¦˜ êµ¬ì¡°ì™€ ì¸í„°í˜ì´ìŠ¤ë§Œ ì„¤ëª…
4. âœ… ê²°ê³¼ ìŠ¤í‚¤ë§ˆ (í‚¤ë§Œ) ë¬¸ì„œí™”

### í…ŒìŠ¤íŠ¸ ê²€ì¦

```python
def test_no_fake_metrics():
    """ê°€ì§œ ë©”íŠ¸ë¦­ ì—†ìŒ í™•ì¸"""
    forbidden_patterns = [
        "trades_total",
        "win_rate",
        "drawdown",
        "pnl",
        "ì˜ˆìƒ ì¶œë ¥"
    ]
    # ì†ŒìŠ¤ ì½”ë“œì—ì„œ íŒ¨í„´ ê²€ìƒ‰
```

---

## í–¥í›„ ê³„íš

### D24: Bayesian Backend êµ¬í˜„

- [ ] Gaussian Process ëª¨ë¸ ì¶”ê°€
- [ ] Acquisition function êµ¬í˜„ (UCB, EI, POI)
- [ ] scikit-opt ë˜ëŠ” hyperopt í†µí•©
- [ ] ì‹¤ì œ Bayesian ìµœì í™” ì‹¤í–‰

### D25: ê³ ê¸‰ ê¸°ëŠ¥

- [ ] Multi-objective optimization
- [ ] Constraint handling
- [ ] Warm start (ì´ì „ ê²°ê³¼ í™œìš©)
- [ ] Early stopping

### D26: ìš´ì˜ ë„êµ¬

- [ ] CLI ëŒ€ì‹œë³´ë“œ
- [ ] ê²°ê³¼ ì‹œê°í™”
- [ ] ë³‘ë ¬ ì‹¤í–‰
- [ ] ë¶„ì‚° íŠœë‹

---

## ê´€ë ¨ ë¬¸ì„œ

- [D22 Tuning Harness](D22_TUNING_HARNESS.md) (í–¥í›„ ì‘ì„±)
- [D21 Observability](D21_OBSERVABILITY_AND_STATE_MANAGER.md)
- [D20 LIVE ARM Guide](D20_LIVE_ARM_GUIDE.md)

---

**ë¬¸ì„œ ì‘ì„±ì:** Cascade AI  
**ìµœì¢… ìˆ˜ì •:** 2025-11-16  
**ìƒíƒœ:** âœ… Production Ready
